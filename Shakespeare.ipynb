{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOsKIAfkbaauDr/boryiLAp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nleck001/CSC221/blob/main/Shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fGuV1yaQZgce"
      },
      "outputs": [],
      "source": [
        "# Created by Orhan G. Yalcin\n",
        "# https://towardsdatascience.com/create-your-own-artificial-shakespeare-in-10-minutes-with-natural-language-processing-1fde5edc8f28\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt',\n",
        "                                       'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IE3l5MnZrGB",
        "outputId": "df4ac17d-e7ca-40a0-ce49-7122fdae9568"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read()\n",
        "text = text.decode(encoding='utf-8')\n",
        "print ('Total number of characters in the corpus is:', len(text))\n",
        "print('The first 100 characters of the corpus are as follows:\\n', text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ervOjatZxLe",
        "outputId": "31722316-c95f-4b2a-eb23-3e3718245bc8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters in the corpus is: 1115394\n",
            "The first 100 characters of the corpus are as follows:\n",
            " First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# The unique characters in the corpus\n",
        "vocab = sorted(set(text))\n",
        "print ('The number of unique characters in the corpus is', len(vocab))\n",
        "print('A slice of the unique characters set:\\n', vocab[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_3RQ-8RaBl_",
        "outputId": "9b908d7b-f976-4678-b0dc-1865ec9720d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of unique characters in the corpus is 65\n",
            "A slice of the unique characters set:\n",
            " ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "# Make a copy of the unique set elements in NumPy array format for later use in the decoding the predictions\n",
        "idx2char = np.array(vocab)\n",
        "# Vectorize the text with a for loop\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "metadata": {
        "id": "SCmR4exQaHUB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "# for i in char_dataset.take(5):\n",
        "#   print(i.numpy())\n",
        "seq_length = 100 # The max. length for single input\n",
        "# examples_per_epoch = len(text)//(seq_length+1) # double-slash for “floor” division\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "# for item in sequences.take(5):\n",
        "#   print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "metadata": {
        "id": "Lw4-ANc_aKvY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "8TNO8CtRaOag"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 10000 # TF shuffles the data only within buffers\n",
        "\n",
        "BATCH_SIZE = 64 # Batch size\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzJCKIQiaXZf",
        "outputId": "dcc00e8b-05f7-46d6-b363-85e46ffb5f3b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n"
      ],
      "metadata": {
        "id": "HQQ3N610aZ7B"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "jacn38Msajbz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(\n",
        "    vocab_size = len(vocab), # no. of unique characters\n",
        "    embedding_dim=embedding_dim, # 256\n",
        "    rnn_units=rnn_units, # 1024\n",
        "    batch_size=BATCH_SIZE)  # 64 for the traning\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8gq4l-Nakmq",
        "outputId": "ea9ad8ab-8927-4d5c-d4e6-6aac33627dff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " gru (GRU)                   (64, None, 1024)          3938304   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4021569 (15.34 MB)\n",
            "Trainable params: 4021569 (15.34 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "# example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "# print(\"Prediction shape: \", example_batch_predictions.shape, \" (batch_size, sequence_length, vocab_size)\")\n",
        "# print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "3t5n68ZBanZ0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "YqAVFNh0auhx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30\n",
        "history = model.fit(dataset,\n",
        "                    epochs=EPOCHS,\n",
        "                    callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShnB1MtgawxJ",
        "outputId": "c323c6c4-9b53-4314-cacb-7835ae95aa1b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "172/172 [==============================] - 24s 58ms/step - loss: 2.6626\n",
            "Epoch 2/30\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.9565\n",
            "Epoch 3/30\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 1.6900\n",
            "Epoch 4/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.5420\n",
            "Epoch 5/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.4538\n",
            "Epoch 6/30\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.3937\n",
            "Epoch 7/30\n",
            "172/172 [==============================] - 14s 60ms/step - loss: 1.3488\n",
            "Epoch 8/30\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 1.3089\n",
            "Epoch 9/30\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 1.2755\n",
            "Epoch 10/30\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.2428\n",
            "Epoch 11/30\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.2087\n",
            "Epoch 12/30\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.1781\n",
            "Epoch 13/30\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.1452\n",
            "Epoch 14/30\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.1110\n",
            "Epoch 15/30\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.0766\n",
            "Epoch 16/30\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.0415\n",
            "Epoch 17/30\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.0056\n",
            "Epoch 18/30\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.9700\n",
            "Epoch 19/30\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.9349\n",
            "Epoch 20/30\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.9025\n",
            "Epoch 21/30\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.8715\n",
            "Epoch 22/30\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.8416\n",
            "Epoch 23/30\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.8152\n",
            "Epoch 24/30\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.7914\n",
            "Epoch 25/30\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.7693\n",
            "Epoch 26/30\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.7511\n",
            "Epoch 27/30\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.7350\n",
            "Epoch 28/30\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.7222\n",
            "Epoch 29/30\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.7075\n",
            "Epoch 30/30\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.6966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eJySIqrHa0ZR",
        "outputId": "5b03bd3c-4873-4b9d-f309-78f82d2e82df"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_30'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZZ5r94cbkqg",
        "outputId": "8f71e9d5-1ee1-420d-8407-bf66629ea717"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (1, None, 256)            16640     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (1, None, 1024)           3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (1, None, 65)             66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4021569 (15.34 MB)\n",
            "Trainable params: 4021569 (15.34 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, num_generate, temperature, start_string):\n",
        "  input_eval = [char2idx[s] for s in start_string] # string to numbers (vectorizing)\n",
        "  input_eval = tf.expand_dims(input_eval, 0) # dimension expansion\n",
        "  text_generated = [] # Empty string to store our results\n",
        "  model.reset_states() # Clears the hidden states in the RNN\n",
        "\n",
        "  for i in range(num_generate): #Run a loop for number of characters to generate\n",
        "    predictions = model(input_eval) # prediction for single character\n",
        "    predictions = tf.squeeze(predictions, 0) # remove the batch dimension\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    # higher temperature increases the probability of selecting a less likely character\n",
        "    # lower --> more predictable\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # The predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    # So the model makes the next prediction based on the previous character\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    # Also devectorize the number and add to the generated text\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "pUlFmYBDbpxV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "generated_text = generate_text(\n",
        "                    model,\n",
        "                    num_generate=500,\n",
        "                    temperature=1,\n",
        "                    start_string=u\"Gentler than thou she wasn't\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MezpzqCbqnH",
        "outputId": "98cf5eb6-660f-487f-be12-a3f9d6681934"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gentler than thou she wasn't\n",
            "Unmiradded his carning chief for Henry Marcius,\n",
            "With what hath something doth sit at Dewishold harmon me not; I saw his head\n",
            "As thou didst see i' the wish of virtue,\n",
            "That wilt not slew them sound;\n",
            "The one by the holsaim of the king\n",
            "In being, whereof we may now.\n",
            "\n",
            "Chouse\n",
            "and thankful to't; tell use me fight and made\n",
            "Thy man of Mint thou depart,\n",
            "Therefore deliver you. O, my father's servais made\n",
            "The tyrannous thievy comfort from me. This is\n",
            "The precedent colours of the news i' the way\n",
            "Whom I would\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jqqRb7rAdP4r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}